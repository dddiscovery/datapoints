<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="https://dddiscovery.github.io/datapoints/feed.xml" rel="self" type="application/atom+xml" /><link href="https://dddiscovery.github.io/datapoints/" rel="alternate" type="text/html" /><updated>2025-03-10T14:08:33+00:00</updated><id>https://dddiscovery.github.io/datapoints/feed.xml</id><title type="html">Data Points</title><subtitle>DDDI&apos;s Technical Blog Series</subtitle><entry><title type="html">Accelerating science with ‚Äòoff-the-shelf‚Äô models</title><link href="https://dddiscovery.github.io/datapoints/off-the-shelf-ai/" rel="alternate" type="text/html" title="Accelerating science with ‚Äòoff-the-shelf‚Äô models" /><published>2025-03-10T00:00:00+00:00</published><updated>2025-03-10T00:00:00+00:00</updated><id>https://dddiscovery.github.io/datapoints/Melanie-Segado</id><content type="html" xml:base="https://dddiscovery.github.io/datapoints/off-the-shelf-ai/"><![CDATA[<p>Not long ago, extracting insights from video data required building custom computer vision models. This was a very time-consuming task that demanded large datasets, hours of researcher time spent hand-annotating data, and lots of computing power. For small datasets like those that are prevalent in many areas of science and medicine, training models was simply not an option altogether. This mismatch between the needs of machine learning and the size of available datasets left many interesting questions unanswered. The goal of my research is to bridge this gap, finding strategies to make the most of small, specialized datasets by analyzing them with foundation models.</p>

<h2 id="why-do-small-datasets-matter">Why do small datasets matter?</h2>

<p>Let‚Äôs consider a concrete example of where this can be useful. Subtle patterns in infant movement during the first months of life can help predict neurodevelopmental disorders such as <strong>cerebral palsy</strong> ‚Äì a lifelong condition affecting balance and motor control. While clinicians excel at picking up these patterns, they don‚Äôt always get to see infants until they‚Äôre much older, and so infants miss out on a critical window of neuroplasticity where rehabilitation would be most helpful. However, videos of infants can be easily recorded using everyday tools like cell-phone cameras and their movements can be analyzed using computer vision, paving the way for the development of low-cost clinical tools that can be implemented at scale.</p>

<p>A big barrier to developing these tools has been the fact that infants are really difficult to analyze from a computer‚Äôs perspective. They tend to bunch up into complicated shapes (occluding their own limbs in the process),and they are often wearing clothes that look a lot like the blankets they‚Äôre laying on. To overcome these difficulties, researchers either had to train custom models for infant pose tracking, or fine-tuning existing algorithms, both of which required lots of time spent annotating videos. But look at how much things have improved over the past 7 years!</p>

<p>Take a look at the video below and move the slider back and forth to compare the performance algorithm from the not-so-distant past (2017) with one from 2023, on a fully ai-generated video of an infant.</p>

<div id="video-compare-container">

  <video id="video1" loop="" muted="" autoplay="" poster="../assets/post_assets/2025-03-01-Melanie-Segado/openpose.jpg">
    <source src="../assets/post_assets/2025-03-01-Melanie-Segado/openpose.mp4" type="video/mp4" />
    <source src="../assets/post_assets/2025-03-01-Melanie-Segado/openpose.webm" type="video/webm" />
  </video>
  <div id="video-clipper">
    <video id="video2" loop="" muted="" autoplay="" poster="../assets/post_assets/2025-03-01-Melanie-Segado/vitposeh.jpg">
      <source src="../assets/post_assets/2025-03-01-Melanie-Segado/vitposeh.mp4" type="video/mp4" />
      <source src="../assets/post_assets/2025-03-01-Melanie-Segado/vitposeh.webm" type="video/webm" />
    </video>
  </div>
  <!-- Initial Center Line with Arrows -->
  <div id="initial-line">
    <div class="arrow" id="left-arrow">‚óÄ</div>
    <div class="arrow" id="right-arrow">‚ñ∂</div>
  </div>
  <div id="slider-line">
    <div class="year" id="dynamic-year-old">2017</div>
    <div class="year" id="dynamic-year-new">2023</div>
    <div class="arrow" id="dynamic-left-arrow">‚óÄ</div>
    <div class="arrow" id="dynamic-right-arrow">‚ñ∂</div>
  </div>
</div>

<div id="button-container">
    <button id="pause-btn" class="cute-button">Pause</button>
</div>

<style>
  .cute-button{
    background:rgb(23, 235, 242); /* Soft turquoise */
    border: none;
    padding: 12px 24px;
    border-radius: 20px; /* Makes it rounded */
    font-size: 16px;
    font-weight: bold;
    color: white;
    cursor: pointer;
    box-shadow: 2px 2px 10px rgba(0, 0, 0, 0.2);
    transition: all 0.3s ease;
  }

  #video-compare-container {
    display: inline-block;
    line-height: 0;
    position: relative;
    width: 100%;
    padding-top: 42.3%;
  }

  #video-compare-container > video {
    width: 100%;
    position: absolute;
    top: 0;
    height: 100%;
  }

  #video-clipper {
    width: 50%;
    position: absolute;
    top: 0;
    bottom: 0;
    overflow: hidden;
  }

  #video-clipper video {
    width: 200%;
    position: absolute;
    height: 100%;
  }

  /* Style the Pause Button */
  #pause-btn {
    display: block;
    margin: 20px auto;
    padding: 10px 20px;
    font-size: 16px;
    cursor: pointer;
  }

  /* Initial Center Line */
  #initial-line {
    position: absolute;
    top: 0;
    left: 50%;
    width: 3px;
    height: 100%;
    background: white;
    transform: translateX(-50%);
    z-index: 10;
    transition: opacity 0.3s ease-in-out;
  }

  /* Vertical Slider Line */
  #slider-line {
    position: absolute;
    top: 0;
    width: 3px;
    height: 100%;
    background: white;
    pointer-events: none;
    z-index: 10;
    display: none;
  }

  /* Arrow Styles */
  .arrow {
    position: absolute;
    top: 50%;
    transform: translateY(-50%);
    font-size: 18px;
    color: white;
    font-weight: bold;
    background: rgba(0, 0, 0, 0);
    padding: 5px;
    border-radius: 50%;
    pointer-events: none;
  }

 .year{
    position: absolute;
    top: 10%;
    transform: translateY(-50%);
    font-size: 18px;
    color: white;
    font-weight: bold;
    background: rgba(0, 0, 0, 0);
    padding: 5px;
    border-radius: 50%;
    pointer-events: none;
 }

  #dynamic-year-old{
    right: -80px;

  }  
  #dynamic-year-new{
    left: -80px;
  }
  #left-arrow, #dynamic-left-arrow {
    left: -20px;
  }

  #right-arrow, #dynamic-right-arrow {
    right: -20px;
  }

  /* Position the arrows for the initial line */
  #left-arrow {
    left: -30px;
  }

  #right-arrow {
    right: -30px;
  }

</style>

<script>
document.addEventListener("DOMContentLoaded", function () {
    var videoContainer = document.getElementById("video-compare-container"),
        video1 = document.getElementById("video1"),
        video2 = document.getElementById("video2"),
        videoClipper = document.getElementById("video-clipper"),
        initialLine = document.getElementById("initial-line"),
        sliderLine = document.getElementById("slider-line"),
        pauseBtn = document.getElementById("pause-btn");

    if (!video1 || !video2) {
        console.error("Error: One or more videos are missing.");
        return;
    }

    // Function to resync videos while keeping playback state
    function resyncVideos() {

        // Force both videos to the same timestamp
        video1.currentTime = video1.currentTime;
        video2.currentTime = video1.currentTime;

    }

    // Pause and Play Functionality (Button)
    pauseBtn.addEventListener("click", function () {
        if (video1.paused || video2.paused) {
            resyncVideos();
            video1.play();
            video2.play();
            pauseBtn.textContent = "Pause";
        } else {
            video1.pause();
            video2.pause();
            resyncVideos();
            pauseBtn.textContent = "Play";
        }
    });

    // Hide initial line when user interacts
    videoContainer.addEventListener("mouseenter", function () {
        initialLine.style.opacity = "0"; // Fade out initial line
    });

    videoContainer.addEventListener("mouseleave", function () {
        initialLine.style.opacity = "1"; // Fade it back in when mouse leaves
    });

    // Track Mouse Movement to Adjust Clipper Width and Show Dynamic Line
    videoContainer.addEventListener("mousemove", function (e) {
        var rect = videoContainer.getBoundingClientRect(),
            position = ((e.pageX - rect.left) / videoContainer.offsetWidth) * 100;

        if (position <= 100) { 
            videoClipper.style.width = position + "%";
            video2.style.width = ((100 / position) * 100) + "%";
            video2.style.zIndex = 3;

            // Show and move the dynamic vertical line
            sliderLine.style.display = "block";
            sliderLine.style.left = e.pageX - rect.left + "px";
        }
    });

    // Hide dynamic line when mouse leaves
    videoContainer.addEventListener("mouseleave", function () {
        sliderLine.style.display = "none";
    });
});


</script>

<p>On the bottom layer (when the slider is all the way to the left) is a model called <a href="https://doi.org/10.48550/arXiv.1812.08008">OpenPose</a>, which was a game-changer in the field when it released. The specific model shown here was pre-trained on 64K images, and finetuned on 47K annotated frames of infant video. While it performs very well when the infant‚Äôs limbs are clearly visible, it fails in spots where the relevant parts of the image are covered by objects like the crib. This is because it‚Äôs relying on finding parts of the image that <em>look like specific joints</em> (e.g., knees). When they‚Äôre occluded, the algorithm fails.</p>

<p>The model overlayed on top (when the slider is all the way to the right) is <a href="https://doi.org/10.48550/arXiv.2204.12484">ViTPose-H</a>, a model pre-trained on vast amounts of image data (300M labelled images), fine-tuned on a much smaller dataset of human poses (250K), and not fine-tuned at all on infant data. As you can see, it does much better at capturing the overall shape of the infant‚Äôs pose, even when the information is occluded, despite not having been trained on infants. This not only has it been trained on more data, it also uses a modern achitechture called a <strong><a href="https://doi.org/10.48550/arXiv.2010.11929">Vision Transformer</a></strong> that enables it to learn not just <em>what specific joints look like</em>, but also <em>their spatial relationship</em> to other joints (e.g., hips and ankles) and other parts of the image.</p>

<h2 id="why-would-these-models-help-accelerate-science">Why would these models help ‚Äúaccelerate science‚Äù?</h2>

<p>Foundation models, pre-trained on massive datasets, have transformed AI applications‚Äîfrom large language models to computer vision. Platforms like <a href="https://huggingface.co/">HuggingFace</a> that host pre-trained models, and user-friendly tools like OpenMMLab‚Äôs <a href="https://mmpose.readthedocs.io/en/latest/overview.html">MMPose</a>, make these powerful tools easily accessible. By fine-tuning pre-trained models with domain-specific data, or even using them straight <em>off the shelf</em>, researchers can achieve meaningful insights with far less effort and fewer resources. It‚Äôs hard to overstate just how rapidly this landscape has evolved. To give an example, the first year of my postdoc was spent optimizing algorithms for infant pose estimation by carefully curating a database of difficult-to-detect poses, annotating them by hand, and training an algorithm to improve its performance. While I made progress, ViTPose performed better off-the-shelf than any of the custom models I had been working on. The ability to get precise pose tracking without the need to train models significantly lowers the barrier to entry for research groups with interesting questions and small, specialized datasets.</p>

<h2 id="how-much-data-is-in-a-massive-dataset">How much data is in a ‚Äúmassive dataset‚Äù</h2>

<p>To get a sense of just how much data goes into pre-training a foundation model that can then be quickly fine-tuned for specialzed applications, let‚Äôs take a look at the relative scale of a few of the datasets that went into training the models used for pose-tracking in the videos above.</p>

<script src="https://d3js.org/d3.v7.min.js"></script>

<style>
    #container {
        position: relative;
        width: 300px;
        height: 300px;
        border: 2px solid #333;
        overflow: hidden;
    }
    .dataset {
        position: absolute;
        background-color: rgba(0, 123, 255, 0.7);
        border: 1px solid #0056b3;
        color: #fff;
        display: flex;
        justify-content: center;
        align-items: center;
        font-size: 10px;
        text-align: center;
    }
    .layout {
        display: flex;
        align-items: center;
        gap: 20px;
    }
    #buttons {
        display: flex;
        flex-direction: column;
        gap: 10px;
    }
    button {
        padding: 10px;
        font-size: 14px;
        cursor: pointer;
        background-color: #0073e6;
        color: white;
        border: none;
        border-radius: 5px;
    }
    button:hover {
        background-color: #005bb5;
    }
    #info-box {
        margin-top: 20px;
        padding: 10px;
        width: 600px;
        text-align: center;
        font-size: 14px;
        background: white;
        border: 1px solid #ccc;
        box-shadow: 2px 2px 5px rgba(0, 0, 0, 0.2);
        border-radius: 5px;
    }
</style>

<div class="layout">
    <!-- Sidebar Buttons -->
    <div id="buttons">
        <button onclick="drawDatasets('jft300m', 300)">JFT-300M (300M images)</button>
        <button onclick="drawDatasets('mscoco', 10)">MS COCO (330K images)</button>
        <button onclick="drawDatasets('mscoco-person', 9)">MS COCO-Person (250K images)</button>
        <button onclick="drawDatasets('infant-frames', 5)">Infant Frames (47K images)</button>
        <button onclick="resetView()">Reset</button>
    </div>

    <!-- Visualization Container -->
    <div id="container"></div>
</div>

<!-- Dynamic Textbox Below -->
<div id="info-box">Click a button to explore dataset sizes.</div>

<script>
    const datasets = [
        { 
            id: "jft300m", name: "JFT-300M", size: 300000000, width: 300, color: "#1f77b4", children: ["mscoco"], 
            descriptions: [
                "JFT-300M is a massive dataset with 300 million images, used to train powerful AI models. Google‚Äôs proprietary dataset is used in training vision transformers, which can then be fine-tuned for tasks like pose estimation."
            ] 
        },
        { 
            id: "mscoco", name: "MS COCO", size: 330000, width: 10, color: "#ff7f0e", children: ["mscoco-person"], 
            descriptions: [
                "MS COCO is a dataset with 330K images, used for object detection and segmentation. The dataset contains labelled images with 80 different object categories inlcluding humans with keypoint annotations. It is widely used in pose estimation and object detection benchmarks"
            ] 
        },
        { 
            id: "mscoco-person", name: "MS COCO-Person", size: 250000, width: 9, color: "#d62728", children: ["infant-frames"], 
            descriptions: [
                "MS COCO-Person is a subset of COCO focused on human annotations, with 250K images. It contains annotations for keypoints, making it essential for training and testing pose-estimation algorithms. This was used to train the OpenPose model visualized above, and to fine-tune ViTPose"
            ] 
        },
        { 
            id: "infant-frames", name: "Infant Frames", size: 47000, width: 5, color: "#2ca02c", children: [], 
            descriptions: [
                "47K hand-annotated infant movement frames, used to finetune the OpenPose algorithm. This dataset highlights the small scale of data available for many specialized applications."
            ] 
        }
    ];

    let textIndex = {}; // Store which text index is currently displayed

    function drawDatasets(parentId, parentSize) {
        const container = document.getElementById("container");
        const infoBox = document.getElementById("info-box");
        container.innerHTML = ""; // Clear container before redrawing

        let parentDataset = datasets.find(d => d.id === parentId);
        let scaleFactor = 300 / parentSize;

        // Cycle text index
        if (!textIndex[parentId]) textIndex[parentId] = 0;
        let descriptions = parentDataset.descriptions;
        infoBox.innerText = descriptions[textIndex[parentId]];
        textIndex[parentId] = (textIndex[parentId] + 1) % descriptions.length; // Move to next text

        // Draw the selected dataset at full size
        let parentDiv = document.createElement("div");
        parentDiv.classList.add("dataset");
        parentDiv.id = parentDataset.id;
        parentDiv.style.width = "300px";
        parentDiv.style.height = "300px";
        parentDiv.style.backgroundColor = parentDataset.color;
        parentDiv.innerHTML = `${parentDataset.name}<br>(${parentDataset.size.toLocaleString()} images)`;

        container.appendChild(parentDiv);

        // Draw the child datasets inside
        parentDataset.children.forEach(childId => {
            let childDataset = datasets.find(d => d.id === childId);
            let childSize = childDataset.width * scaleFactor;
            
            let childDiv = document.createElement("div");
            childDiv.classList.add("dataset");
            childDiv.id = childDataset.id;
            childDiv.style.width = `${childSize}px`;
            childDiv.style.height = `${childSize}px`;
            childDiv.style.backgroundColor = childDataset.color;
            childDiv.style.bottom = "5px";
            childDiv.style.left = "5px";
            childDiv.innerHTML = `${childDataset.name}<br>(${childDataset.size.toLocaleString()} images)`;

            container.appendChild(childDiv);
        });
    }

    function resetView() {
        const container = document.getElementById("container");
        const infoBox = document.getElementById("info-box");
        container.innerHTML = ""; // Clear container
        infoBox.innerText = "Click a button to explore dataset sizes."; // Reset text

        datasets.forEach(dataset => {
            let div = document.createElement("div");
            div.classList.add("dataset");
            div.id = dataset.id;
            div.style.width = `${dataset.width}px`;
            div.style.height = `${dataset.width}px`;
            div.style.backgroundColor = dataset.color;
            div.style.position = "absolute";
            div.style.bottom = "5px";
            div.style.left = "5px";
            div.innerHTML = `${dataset.name}<br>(${dataset.size.toLocaleString()} images)`;

            container.appendChild(div);
        });

        textIndex = {}; // Reset cycling index
    }

    // Initialize view
    resetView();
</script>

<h2 id="impact-and-future-directions">Impact and Future Directions</h2>

<p>The rise of <em>off-the-shelf</em> AI models marks a significant shift in the ease with which researchers can integrate state-of-the-art tools into their research. <strong>Vision transformers for movement analysis</strong> are just one example of how accessible AI tools can help push the boundaries of disease detection and treatment. As these resources become even more widely available, the future holds breakthroughs that will benefit patients worldwide.</p>

<p>If you want to see how we‚Äôve been able to put this approach to use, check out our pre-prints! 
<a class="paper-title-link" href="https://doi.org/10.1101/2025.02.10.25322007"> Data-Driven Early Prediction of Cerebral Palsy Using AutoML and interpretable kinematic features (medRxiv)</a></p>

<p><a class="paper-title-link" href="https://doi.org/10.1101/2024.11.06.24316844"> Assessing infant risk of cerebral palsy with video-based motion tracking (medRxiv)</a></p>]]></content><author><name>[Melanie Segado](https://sites.google.com/view/melaniesegado)</name></author><summary type="html"><![CDATA[Not long ago, extracting insights from video data required building custom computer vision models. This was a very time-consuming task that demanded large datasets, hours of researcher time spent hand-annotating data, and lots of computing power. For small datasets like those that are prevalent in many areas of science and medicine, training models was simply not an option altogether. This mismatch between the needs of machine learning and the size of available datasets left many interesting questions unanswered. The goal of my research is to bridge this gap, finding strategies to make the most of small, specialized datasets by analyzing them with foundation models.]]></summary></entry><entry><title type="html">Contributor Guide</title><link href="https://dddiscovery.github.io/datapoints/contributor-guide/" rel="alternate" type="text/html" title="Contributor Guide" /><published>2025-02-10T00:00:00+00:00</published><updated>2025-02-10T00:00:00+00:00</updated><id>https://dddiscovery.github.io/datapoints/Contributor-Guide</id><content type="html" xml:base="https://dddiscovery.github.io/datapoints/contributor-guide/"><![CDATA[<!-- Content sections -->
<section id="github-repo">
  <h2 id="github-repo">GitHub Repo</h2>

  <p>The Datapoints blog is built on GitHub, providing a collaborative platform for our team to create and share data science insights. This guide will walk you through our repository structure, development workflow, and help you set up your local environment for contributing to the blog.</p>

  <p>Whether you‚Äôre writing your first post or maintaining existing content, understanding our GitHub workflow will help ensure a smooth contribution process. Let‚Äôs get started!</p>

  <p><img src="/datapoints/assets/images/picture/git-repo.jpg" alt="GitHub Repository" style="max-width: 75%; height: auto; display: block; margin-left: auto; margin-right: auto;" /></p>
</section>

<section id="project-structure">
  <h3 id="project-structure">Project Structure</h3>

  <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DDDI_DP_Blog/
‚îú‚îÄ‚îÄ _layouts/           # Universal HTML layouts for the blog
‚îú‚îÄ‚îÄ _posts/            # Blog post content in Markdown
‚îÇ   ‚îî‚îÄ‚îÄ YYYY-MM-DD-FirstName-LastName.md
‚îú‚îÄ‚îÄ assets/            # Static assets
‚îÇ   ‚îú‚îÄ‚îÄ css/          # Universal CSS files
‚îÇ   ‚îú‚îÄ‚îÄ images/       # Global images
‚îÇ   ‚îî‚îÄ‚îÄ post_assets/  # Post-specific assets
‚îÇ       ‚îî‚îÄ‚îÄ YYYY-MM-DD-FirstName-LastName/  # Matches .md filename
‚îî‚îÄ‚îÄ README.md
</code></pre></div>  </div>

  <h4 id="directory-details">Directory Details</h4>

  <h5 id="layouts"><code class="language-plaintext highlighter-rouge">_layouts/</code></h5>

  <p>Contains universal HTML layouts used across the blog. These templates define the structure and appearance of different page types.</p>

  <h5 id="posts"><code class="language-plaintext highlighter-rouge">_posts/</code></h5>

  <p>Stores all blog post content in Markdown format.</p>
  <ul>
    <li>Naming convention: <code class="language-plaintext highlighter-rouge">YYYY-MM-DD-FirstName-LastName.md</code></li>
    <li>Example: <code class="language-plaintext highlighter-rouge">2025-01-27-Yuxin-Liang.md</code></li>
  </ul>

  <h5 id="assets"><code class="language-plaintext highlighter-rouge">assets/</code></h5>

  <p>Static assets organized in three main categories:</p>
  <ul>
    <li><code class="language-plaintext highlighter-rouge">css/</code>: Universal CSS files applied across the site</li>
    <li><code class="language-plaintext highlighter-rouge">images/</code>: Global images used throughout the blog</li>
    <li><code class="language-plaintext highlighter-rouge">post_assets/</code>: Post-specific assets
      <ul>
        <li>Each post should have its own directory matching the .md filename</li>
        <li>Example: If your post is <code class="language-plaintext highlighter-rouge">2025-01-27-Yuxin-Liang.md</code>, create <code class="language-plaintext highlighter-rouge">assets/post_assets/2025-01-27-Yuxin-Liang/</code></li>
        <li>Store all post-specific settings, images, and other assets in this folder</li>
      </ul>
    </li>
  </ul>
</section>

<section id="branches">
  <h3 id="working-with-branches">Working with Branches</h3>

  <h4 id="branch-structure">Branch Structure</h4>
  <ul>
    <li><code class="language-plaintext highlighter-rouge">main</code>: Production-ready code, final destination for completed posts</li>
    <li><code class="language-plaintext highlighter-rouge">firstname-lastname-staging</code>: Personal development branch for each author</li>
  </ul>

  <h4 id="workflow">Workflow</h4>
  <ol>
    <li>
      <p><strong>Create Your Branch</strong></p>

      <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create and switch to your personal staging branch</span>
git checkout <span class="nt">-b</span> yuxin-liang-staging
</code></pre></div>      </div>
    </li>
    <li>
      <p><strong>Development Process</strong></p>

      <ul>
        <li>Make all your changes in your personal staging branch</li>
        <li>Test locally using Jekyll server</li>
        <li>Commit changes frequently with clear messages</li>
      </ul>
    </li>
    <li>
      <p><strong>Push to Your Branch</strong></p>

      <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git push origin yuxin-liang-staging
</code></pre></div>      </div>
    </li>
    <li>
      <p><strong>Final Review</strong></p>

      <ul>
        <li>Ensure all content is complete</li>
        <li>Test all interactive elements</li>
        <li>Verify formatting across devices</li>
      </ul>
    </li>
    <li>
      <p><strong>Merge to Main</strong></p>

      <ul>
        <li>Only merge to <code class="language-plaintext highlighter-rouge">main</code> when post is complete</li>
        <li>Create pull request from your branch to <code class="language-plaintext highlighter-rouge">main</code></li>
        <li>Request review if needed</li>
      </ul>
    </li>
  </ol>

</section>

<section id="local-setup">
  <h3 id="setting-up-local-environment">Setting Up Local Environment</h3>

  <h4 id="prerequisites">Prerequisites</h4>
  <ol>
    <li><strong>Install Ruby</strong>
      <ul>
        <li>For Windows:
          <ul>
            <li>Download and install Ruby from <a href="https://rubyinstaller.org/">RubyInstaller</a></li>
            <li>Verify installation: <code class="language-plaintext highlighter-rouge">ruby -v</code></li>
          </ul>
        </li>
        <li>For macOS:
          <ul>
            <li>Ruby comes pre-installed, but it‚Äôs recommended to use a version manager</li>
            <li>Install Homebrew if not already installed:
              <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/bin/bash <span class="nt">-c</span> <span class="s2">"</span><span class="si">$(</span>curl <span class="nt">-fsSL</span> https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh<span class="si">)</span><span class="s2">"</span>
</code></pre></div>              </div>
            </li>
            <li>Install rbenv:
              <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>brew <span class="nb">install </span>rbenv
rbenv init
</code></pre></div>              </div>
            </li>
            <li>Install Ruby:
              <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rbenv <span class="nb">install </span>3.2.0
rbenv global 3.2.0
</code></pre></div>              </div>
            </li>
            <li>Verify installation: <code class="language-plaintext highlighter-rouge">ruby -v</code></li>
          </ul>
        </li>
      </ul>
    </li>
    <li><strong>Install Jekyll and Bundler</strong>
      <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>gem <span class="nb">install </span>jekyll bundler
</code></pre></div>      </div>
    </li>
  </ol>

  <h4 id="getting-started">Getting Started</h4>
  <ol>
    <li><strong>Clone the Repository</strong>
      <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/dddiscovery/datapoints.git
<span class="nb">cd </span>datapoints
</code></pre></div>      </div>
    </li>
    <li><strong>Install Dependencies</strong>
      <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bundle <span class="nb">install</span>
</code></pre></div>      </div>
    </li>
    <li><strong>Start Local Server</strong>
      <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bundle <span class="nb">exec </span>jekyll serve
</code></pre></div>      </div>
    </li>
  </ol>

  <h4 id="common-issues">Common Issues</h4>
  <ul>
    <li><strong>Permission Errors During Installation</strong>
      <ul>
        <li>Try installing gems with the <code class="language-plaintext highlighter-rouge">--user-install</code> flag</li>
        <li>Check directory permissions for your gem folder</li>
      </ul>
    </li>
    <li><strong>Bundle Install Failures</strong>
      <ul>
        <li>Run bundle update first</li>
        <li>Clean and reinstall dependencies</li>
        <li>Check for conflicting gem versions</li>
      </ul>
    </li>
    <li><strong>Ruby Version Conflicts</strong>
      <ul>
        <li>Ensure Ruby version matches requirements (3.2.0 recommended)</li>
        <li>Use version managers (rbenv/RVM) to switch Ruby versions</li>
        <li>Check Jekyll compatibility with your Ruby version</li>
      </ul>
    </li>
    <li><strong>Port Already in Use</strong>
      <ul>
        <li>Check for other running Jekyll instances</li>
        <li>Try different port number</li>
        <li>Restart your terminal</li>
      </ul>
    </li>
    <li><strong>Missing Dependencies</strong>
      <ul>
        <li>Install required system libraries</li>
        <li>Check Gemfile for missing entries</li>
        <li>Verify development headers are installed</li>
      </ul>
    </li>
  </ul>

  <h4 id="preview-across-devices-using-ngrok">Preview Across Devices (Using ngrok)</h4>

  <p>ngrok allows you to access your local development site from any device, regardless of network.</p>

  <ol>
    <li><strong>Start Jekyll Server with Host Settings</strong>
      <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>datapoints    <span class="c"># Navigate to your project directory</span>
bundle <span class="nb">exec </span>jekyll serve <span class="nt">--host</span><span class="o">=</span>0.0.0.0 <span class="nt">--baseurl</span><span class="o">=</span><span class="s2">""</span>
</code></pre></div>      </div>
      <p>Keep this terminal window open!</p>
    </li>
    <li><strong>Start ngrok in New Terminal</strong>
      <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ngrok http 4000
</code></pre></div>      </div>
      <p>Keep this terminal window open too!</p>
    </li>
    <li><strong>Get the Public URL</strong>
      <ul>
        <li>Look for the ‚ÄòForwarding‚Äô line in ngrok output</li>
        <li>Use the <code class="language-plaintext highlighter-rouge">https://</code> URL (example: <code class="language-plaintext highlighter-rouge">https://xxxx-xxxx.ngrok-free.app</code>)</li>
      </ul>
    </li>
    <li><strong>Access on Other Devices</strong>
      <ul>
        <li>Open the ngrok URL on any device</li>
        <li>Site will be accessible regardless of network</li>
        <li>Great for testing responsive design</li>
        <li>Share URL with collaborators for quick previews</li>
      </ul>
    </li>
  </ol>

  <h4 id="notes">Notes</h4>
  <ul>
    <li>ngrok URLs are temporary and change each session</li>
    <li>Free tier has some limitations</li>
    <li>For security, only share URLs with trusted collaborators</li>
  </ul>

</section>

<section id="page-deployment">
  <h3 id="github-pages-deployment">GitHub Pages Deployment</h3>

  <h4 id="build-process">Build Process</h4>

  <p>After completing all content and styling, pushing changes to the <code class="language-plaintext highlighter-rouge">main</code> branch triggers an automated build process.</p>

  <ol>
    <li><strong>GitHub Action</strong>
      <ul>
        <li>A GitHub Action called ‚ÄúDeploy Jekyll Site‚Äù is triggered</li>
        <li>You can monitor the build at: https://github.com/dddiscovery/datapoints/actions</li>
      </ul>
    </li>
    <li><strong>Build Time</strong>
      <ul>
        <li>The build process takes a few minutes to complete</li>
        <li>You can track progress in the Actions tab</li>
        <li>A green checkmark ‚úÖ indicates successful deployment</li>
      </ul>
    </li>
    <li><strong>Deployment</strong>
      <ul>
        <li>The built site is automatically pushed to the <code class="language-plaintext highlighter-rouge">gh-pages</code> branch</li>
        <li>Once deployed, your changes will be live at: https://dddiscovery.github.io/datapoints</li>
      </ul>
    </li>
  </ol>

  <h4 id="troubleshooting">Troubleshooting</h4>

  <ul>
    <li>If the build fails (red ‚ùå), check the Actions tab for error details</li>
    <li>Common issues include:
      <ul>
        <li>Invalid front matter in posts</li>
        <li>Missing required files</li>
        <li>Incorrect file paths</li>
        <li>Build timeout or resource limits exceeded</li>
        <li>CSS or JavaScript not loading properly</li>
      </ul>
    </li>
  </ul>

  <h4 id="important-notes">Important Notes</h4>

  <ul>
    <li>Only changes pushed to <code class="language-plaintext highlighter-rouge">main</code> trigger the build process</li>
    <li>The build configuration lives in <code class="language-plaintext highlighter-rouge">.github/workflows/pages.yml</code></li>
    <li>Wait for the green checkmark before checking your changes on the live site</li>
  </ul>

</section>

<section id="for-author">
  <h3 id="ready-set-blog">Ready, Set, Blog!</h3>

  <ol>
    <li><strong>Create a new branch for your post</strong>
      <ul>
        <li>Branch naming convention: <code class="language-plaintext highlighter-rouge">post/YYYY-MM-DD-FirstName-LastName</code></li>
        <li>Example: <code class="language-plaintext highlighter-rouge">post/2025-01-27-Yuxin-Liang</code></li>
      </ul>
    </li>
    <li><strong>Create your blog post in <code class="language-plaintext highlighter-rouge">_posts/</code> following the naming convention</strong>
      <ul>
        <li>Create a new .md file named <code class="language-plaintext highlighter-rouge">YYYY-MM-DD-FirstName-LastName.md</code></li>
        <li>Use the template in <code class="language-plaintext highlighter-rouge">2025-01-22-post-temp.md</code> as a starting point</li>
        <li>Write your blog content in Markdown format</li>
      </ul>
    </li>
    <li><strong>Create a matching directory in <code class="language-plaintext highlighter-rouge">assets/post_assets/</code> for your specific assets</strong>
      <ul>
        <li>Directory should match your .md filename: <code class="language-plaintext highlighter-rouge">assets/post_assets/YYYY-MM-DD-FirstName-LastName/</code></li>
        <li>Place all post-specific content (images, CSS, scripts) in this directory</li>
      </ul>
    </li>
    <li><strong>Work on your staging branch</strong>
      <ul>
        <li>Make all changes and test locally (follow the local development setup below)</li>
        <li>Commit your changes regularly with descriptive messages</li>
      </ul>
    </li>
    <li><strong>Review and publish</strong>
      <ul>
        <li>Request a review when your post is ready</li>
        <li>After approval, merge your branch into main</li>
      </ul>
    </li>
  </ol>

</section>

<section id="git-resources">
  <h3 id="git-resources">Git Resources</h3>

  <p>Lost in the land of commits and how git works ü§Ø? Here‚Äôs your survival guide to version control.</p>

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><a href="https://docs.github.com/en/get-started">GitHub Docs</a></td>
        <td>The official GitHub documentation - comprehensive, clear, and constantly updated.</td>
      </tr>
      <tr>
        <td><a href="https://git-scm.com/book/en/v2">Pro Git Book</a></td>
        <td>Free book covering everything from Git basics to advanced usage.</td>
      </tr>
      <tr>
        <td><a href="https://www.atlassian.com/git/tutorials">Atlassian Git Tutorials</a></td>
        <td>Covers Git workflows, branching, rebasing, and best practices.</td>
      </tr>
    </tbody>
  </table>

</section>

<section id="blog-inspirations">
  <h2 id="blog-inspirations">Blog Inspirations</h2>

  <p>The Datapoints blog aims to make data science accessible, engaging, and visually compelling. We‚Äôve curated a collection of resources that inspire our creative approach to data storytelling and visualization. These exemplary works showcase how complex ideas can be communicated clearly and beautifully.</p>

  <p>Whether you‚Äôre looking for visualization ideas, narrative techniques, or interactive elements, these resources will help elevate your blog posts from informative to unforgettable. Let‚Äôs explore what makes great data stories!</p>

  <p>We‚Äôd love to hear about any inspiring resource that could benefit our community! Share your findings with us, and let‚Äôs grow our collection of inspirational resources together.</p>

  <p><img src="/datapoints/assets/images/picture/blog-inspirations.jpg" alt="GitHub Repository" style="max-width: 100%; height: auto;" /></p>
</section>

<section id="creative-viz">
  <h3 id="creative-visualization">Creative VIsualization</h3>

  <p>Cool examples of how to turn data and complex ideas into engaging visuals and stories. These sites show how to make your research pop with creative, interactive elements.</p>

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><a href="https://visxai.io/">VISxAI</a></td>
        <td>A workshop showcasing creative ways to explain AI through interactive visualizations.</td>
      </tr>
      <tr>
        <td><a href="https://pudding.cool/">The Pudding</a></td>
        <td>Visual essays that turn data into delightful stories.</td>
      </tr>
      <tr>
        <td><a href="https://seeing-theory.brown.edu/index.html">Seeing Theory</a></td>
        <td>A stunning visual journey through statistics and probability.</td>
      </tr>
    </tbody>
  </table>

</section>

<section id="scientific-journals">
  <h3 id="scientific-journals">Scientific Journals</h3>
  <p>Science with style - research that reads like a page-turner. A collection of blogs and publications that nail the sweet spot between rigorous research and engaging storytelling. <span style="text-decoration:line-through;font-size:1.125rem;font-family:Palatino,URW Palladio L,serif;color:#666666">Because let‚Äôs face it - most academic papers are a cure for insomnia!</span></p>

  <table>
    <thead>
      <tr>
        <th>Resource</th>
        <th>Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><a href="https://distill.pub/">Distill</a></td>
        <td>A pioneer in interactive ML explanations.</td>
      </tr>
      <tr>
        <td><a href="https://pair.withgoogle.com/explorables/">PAIR Explorables</a></td>
        <td>Google‚Äôs collection of interactive essays that make ML concepts tangible.</td>
      </tr>
      <tr>
        <td><a href="https://www.quantamagazine.org/">Quanta Magazine</a></td>
        <td>A blog series explores cutting-edge science and mathematics through in-depth, accessible journalism.</td>
      </tr>
    </tbody>
  </table>

</section>

<section id="art-credits">
  <h2 id="artwork-credits">Artwork Credits</h2>

  <p>The blog features artwork from the public domain for both title images and other visual elements throughout the site:</p>

  <div class="credits-container">

    <h3 id="title-images">Title Images</h3>
    <ul>
      <li><em>‚ÄúFlecks of Foam‚Äù</em>
        <ul>
          <li>Artist: Henry Golden Dearth</li>
          <li>Accession Number: 1963.10.120</li>
          <li>Source: Chester Dale Collection</li>
          <li>Status: Public Domain</li>
        </ul>
      </li>
    </ul>

    <ul>
      <li><em>‚ÄúCannon Rock‚Äù</em>
        <ul>
          <li>Artist: Winslow Homer</li>
          <li>Accession Number: 06.1281</li>
          <li>Source: The Metropolitan Museum of Art</li>
          <li>Status: Public Domain</li>
        </ul>
      </li>
    </ul>

  </div>

  <div class="credits-container">

    <h3 id="other-images">Other Images</h3>
    <ul>
      <li><em>‚ÄúLondon Bridge‚Äù</em>
        <ul>
          <li>Artist: Andr√© Derain</li>
          <li>Accession Number: 195.1952</li>
          <li>Source: Museum of Modern Art (MoMA)</li>
          <li>Status: Public Domain</li>
        </ul>
      </li>
    </ul>

    <ul>
      <li><em>‚ÄúThe Bathing Hour, Chester, Nova Scotia‚Äù</em>
        <ul>
          <li>Artist: William James Glackens</li>
          <li>Accession Number: BF149</li>
          <li>Source: Barnes Collection</li>
          <li>Status: Public Domain</li>
        </ul>
      </li>
    </ul>

    <ul>
      <li><em>‚ÄúJockeys and Race Horses‚Äù</em>
        <ul>
          <li>Artist: Edgar Degas</li>
          <li>Accession Number: BF572</li>
          <li>Source: Barnes Collection</li>
          <li>Status: Public Domain</li>
        </ul>
      </li>
    </ul>

    <ul>
      <li><em>‚ÄúThe Dance Class‚Äù</em>
        <ul>
          <li>Artist: Edgar Degas</li>
          <li>Object Number: 1987.47.1</li>
          <li>Source: The Metropolitan Museum of Art</li>
          <li>Status: Public Domain</li>
        </ul>
      </li>
    </ul>

    <ul>
      <li><em>‚ÄúThe Monet Family in Their Garden at Argenteuil‚Äù</em>
        <ul>
          <li>Artist: Edouard Manet</li>
          <li>Object Number: 1976.201.14</li>
          <li>Source: The Metropolitan Museum of Art</li>
          <li>Status: Public Domain</li>
        </ul>
      </li>
    </ul>

  </div>

  <p><span style="color: #990000"><em>Note: All artwork used as title images are selected from the public domain to ensure proper usage rights.</em></span></p>

</section>]]></content><author><name></name></author><summary type="html"><![CDATA[GitHub Repo]]></summary></entry><entry><title type="html">Where is the information in data?</title><link href="https://dddiscovery.github.io/datapoints/Where-is-the-Information-in-Data/" rel="alternate" type="text/html" title="Where is the information in data?" /><published>2025-02-03T00:00:00+00:00</published><updated>2025-02-03T00:00:00+00:00</updated><id>https://dddiscovery.github.io/datapoints/Kieran-Murphy</id><content type="html" xml:base="https://dddiscovery.github.io/datapoints/Where-is-the-Information-in-Data/"><![CDATA[<p>Imagine you have a special X-ray-like device that you can point toward a dataset and then it illuminates the information contained within.  What sorts of things could you learn?  What would the illuminated information even look like?</p>

<p>My research is about building such a device with machine learning, and then finding ways to make what it finds interpretable. 
In this post, we‚Äôll dig into a small example where you try to predict the rate of bike rentals based on some time descriptors, derived from the classic <a href="https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset"><em>Bikeshare</em> dataset</a>.  Rather than using the full set of 12 descriptors, we‚Äôll pick four to make things simpler.  The four are:</p>

<ul>
  <li>the year the data was collected (2011 or 2012, in Washington, D.C.)</li>
  <li>the season (winter, spring, summer, or fall)</li>
  <li>the day of the week</li>
  <li>the hour of the day</li>
</ul>

<p>The dataset is a bunch of records of these four descriptors along with the number of bikes rented.
Inside the dataset is information about the relationship between the descriptors‚Äô values and bike rentals, and this information is what we are after.
By <em>information</em> we mean a quantity from the formalism of <a href="https://en.wikipedia.org/wiki/Information_theory">information theory</a>, but you can think of it as a clue that allows for better predictions.</p>

<p>Let‚Äôs point our special device at the dataset, and illuminate the information!  You can hover over each descriptor to highlight the colored curve that it corresponds to.</p>

<div class="sticky-container">
<div class="tabular-decomp row sticky"></div>

<br />

At first glance, this might not look like what you‚Äôd imagine 'information' to be. Don‚Äôt worry, what you‚Äôre seeing is just the beginning. Let‚Äôs break it down. <br /><br />

The plot above is about a spectrum of predictive models found with machine learning, and each model uses a different amount of information about the input descriptors to do the best possible job predicting the number of bikes rented in a given hour.  On the left hand side of the plot is a model that uses no information, and its error (in black) is the worst of the models in the spectrum.  On the right hand side is a model that uses around 10 bits of information, and its error is the best we can attain with these four descriptors.  Along the way, information is extracted in order of importance to the relationship between the descriptors and the number of bike rentals.  In other words, asking "where is the information in the dataset?" is best answered with a prioritization of details: "Something is the most important information, then another thing, and then something else." <br /><br />
 
So, what is the most important information?  The colored curves show the amount of information extracted from each input descriptor.  The red curve displays the amount of information (with the scale shown on the right vertical axis) pulled from the hour of the day, and we can tell it has the most information about bike rentals.  Then the other three descriptors contribute later and to a lesser degree, with season being the next most important after hour.  That's pretty interesting already, but is there more to the story?<br /><br />

To dig deeper will require recognizing that information allows you to make distinctions between things.  An example of a partial piece of information about the hour of the day would tell you whether it's morning or afternoon; from this, a model trying to predict bike rentals would be able to distinguish 8am from 8pm, but could not distinguish 7pm from 8pm.  
Without being able to make that distinction, the model would have to output the same prediction for all hours of the afternoon.  With more information, the predictions can become more fine-grained.<br /><br />

Below we show the information extracted from the four descriptors in terms of distinctions between their values, as a function of the total information extracted.  The distinctions are between pairs of inputs, so there is a square for the ability to distinguish winter from spring, another to distinguish winter from summer, and so on.  Entries are white if the feature values are indistinguishable, meaning no information was passed along to distinguish the values, and shades of blue depending on the degree of distinguishability.<a class="footstart" key="bhat"></a><br /><br />

</div>

<div class="container">
  <div class="compression-level-slider"></div>
</div>
<div class="distinguishability-mats row" width="50"></div>

<p>The most relevant information in all the descriptors is about the hour of the day, and roughly groups the hours of the day into nighttime, commuting hours, and everything else.</p>

<p>The next most relevant information (found by sliding the bar to the right) distinguishes winter from the warmer months, distinguishes the two years of data from each other, and distinguishes the weekend from the weekdays.  These specific distinctions are the information lying inside the data about the relationship between time and bike rentals.</p>

<p>Eventually, more nuanced details from the descriptors is selected, and then it becomes easier to interpret what is left <em>indistinguishable</em>: spring from fall, the hours from 2am-6am, and the days Tuesday-Thursday.</p>

<h3 id="parting-thoughts">Parting thoughts</h3>

<p>Data is essentially just a bunch of variation, and one of the central challenges in data science is finding the right variation to focus on.  In this example, the right variation was that which was most informative about bike rentals.
Machine learning identified the information for us, guiding our focus and allowing for our own mental modeling to kick in.</p>

<p>We focused on a small example with only four descriptors and a single output value to predict, but localizing information can scale gracefully to much larger problems, and less straightforward prediction setups.
If you‚Äôre interested in learning more, check out the longer post that this one came from: <a href="https://murphyka.github.io/information_explorable/">Where is the information in data?</a>, and our publications on the topic:</p>

<p><a class="paper-title-link" href="https://arxiv.org/abs/2211.17264">Interpretability with full complexity by constraining feature information (ICLR 2023)</a></p>

<p><a class="paper-title-link" href="https://www.pnas.org/doi/abs/10.1073/pnas.2312988121">Information decomposition in complex systems via machine learning (PNAS 2024)</a></p>

<p><a class="footend" key="bhat"></a> 
Specifically, we use the <a href="https://en.wikipedia.org/wiki/Bhattacharyya_distance">Bhattacharyya coefficient</a> between the probability distributions of these values in representation space, and it is 1 when the representations perfectly overlap (white) and 0 when they have no overlap (dark blue).</p>

<link rel="stylesheet" href="/datapoints/assets/post_assets/2025-01-13-Kieran-Murphy/footnote_v2.css" />

<link rel="stylesheet" href="/datapoints/assets/post_assets/2025-01-13-Kieran-Murphy/citation_v2.css" />

<link rel="stylesheet" href="/datapoints/assets/post_assets/2025-01-13-Kieran-Murphy/style.css" />

<script id="MathJax-script" async="" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

<script defer="" src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/mathtex-script-type.min.js" integrity="sha384-jiBVvJ8NGGj5n7kJaiWwWp9AjC+Yh8rhZY3GtAX8yU28azcLgoRo4oukO87g7zDT" crossorigin="anonymous"></script>

<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest/dist/tf.min.js"></script>

<script src="/datapoints/assets/post_assets/2025-01-13-Kieran-Murphy/third_party/d3_.js"></script>

<script src="/datapoints/assets/post_assets/2025-01-13-Kieran-Murphy/third_party/d3-scale-chromatic.v1.min.js"></script>

<script src="/datapoints/assets/post_assets/2025-01-13-Kieran-Murphy/third_party/npyjs-global.js"></script>

<script src="/datapoints/assets/post_assets/2025-01-13-Kieran-Murphy/footnote_v2.js"></script>

<script src="/datapoints/assets/post_assets/2025-01-13-Kieran-Murphy/citation_v2.js"></script>

<script src="/datapoints/assets/post_assets/2025-01-13-Kieran-Murphy/util.js"></script>

<script src="/datapoints/assets/post_assets/2025-01-13-Kieran-Murphy/init-input-sliders.js"></script>

<link rel="stylesheet" href="/datapoints/assets/post_assets/2025-01-13-Kieran-Murphy/tabular/style.css" />

<script src="/datapoints/assets/post_assets/2025-01-13-Kieran-Murphy/tabular/init.js"></script>

<script src="/datapoints/assets/post_assets/2025-01-13-Kieran-Murphy/tabular/init-distinguishability.js"></script>

<script src="/datapoints/assets/post_assets/2025-01-13-Kieran-Murphy/init-info-plane.js"></script>]]></content><author><name>[Kieran Murphy](https://www.kieranamurphy.com/)</name></author><summary type="html"><![CDATA[Imagine you have a special X-ray-like device that you can point toward a dataset and then it illuminates the information contained within. What sorts of things could you learn? What would the illuminated information even look like?]]></summary></entry><entry><title type="html">your post title</title><link href="https://dddiscovery.github.io/datapoints/Your-Post-Title/" rel="alternate" type="text/html" title="your post title" /><published>2025-01-22T00:00:00+00:00</published><updated>2025-01-22T00:00:00+00:00</updated><id>https://dddiscovery.github.io/datapoints/post-temp</id><content type="html" xml:base="https://dddiscovery.github.io/datapoints/Your-Post-Title/"><![CDATA[<p>‚ÄúYour content here‚Äù</p>]]></content><author><name>[author 1](https://author home page/)</name></author><summary type="html"><![CDATA[‚ÄúYour content here‚Äù]]></summary></entry><entry><title type="html">Data Points</title><link href="https://dddiscovery.github.io/datapoints/" rel="alternate" type="text/html" title="Data Points" /><published>2024-12-31T00:00:00+00:00</published><updated>2024-12-31T00:00:00+00:00</updated><id>https://dddiscovery.github.io/datapoints/Data-Points</id><content type="html" xml:base="https://dddiscovery.github.io/datapoints/"><![CDATA[]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry></feed>